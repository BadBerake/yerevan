#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
2GIS Selenium Scraper for Yerevan Cafes
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±ÙˆØ§Ù† Ø§Ø² 2GIS Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Browser Automation

Ø§Ø³ØªÙØ§Ø¯Ù‡:
1. Ù†ØµØ¨ dependencies: pip install -r requirements.txt
2. Ø§Ø¬Ø±Ø§: python 2gis_selenium_scraper.py
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import csv
import time
from typing import List, Dict, Any
import os

# ========== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ==========
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± ChromeDriver Ø¨Ù‡ PATH
os.environ['PATH'] = os.path.expanduser('~/bin') + os.pathsep + os.environ.get('PATH', '')

BASE_URL = "https://2gis.am/yerevan"
SEARCH_QUERY = "cafe"
OUTPUT_JSON = "yerevan_cafes_selenium.json"
OUTPUT_CSV = "yerevan_cafes_selenium.csv"
MAX_CAFES = 10  # ØªØ³Øª Ø³Ø±ÛŒØ¹: 10 Ú©Ø§ÙÙ‡


def setup_driver():
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø±ÙˆØ±Ú¯Ø± Chrome"""
    chrome_options = Options()
    chrome_options.add_argument("--start-maximized")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ headless (Ø¨Ø¯ÙˆÙ† Ù†Ù…Ø§ÛŒØ´ Ù…Ø±ÙˆØ±Ú¯Ø±)ØŒ Ø®Ø· Ø²ÛŒØ± Ø±Ø§ uncomment Ú©Ù†ÛŒØ¯
    # chrome_options.add_argument("--headless")
    
    driver = webdriver.Chrome(options=chrome_options)
    return driver


def search_cafes(driver: webdriver.Chrome) -> bool:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ Ø¯Ø± 2GIS
    
    Args:
        driver: WebDriver Ù…Ø±ÙˆØ±Ú¯Ø±
        
    Returns:
        True Ø§Ú¯Ø± Ø¬Ø³ØªØ¬Ùˆ Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´Ø¯ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª False
    """
    try:
        print("ğŸŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ 2GIS...")
        driver.get(BASE_URL)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ - timeout Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡
        wait = WebDriverWait(driver, 30)  # Ø§Ø² 15 Ø¨Ù‡ 30 Ø«Ø§Ù†ÛŒÙ‡ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØª
        
        print("â³ Ù…Ù†ØªØ¸Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡...")
        time.sleep(5)  # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„
        
        # Ø°Ø®ÛŒØ±Ù‡ screenshot Ø¨Ø±Ø§ÛŒ debug
        driver.save_screenshot("debug_page_loaded.png")
        print("ğŸ“¸ Screenshot Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: debug_page_loaded.png")
        
        # ÛŒØ§ÙØªÙ† Ú©Ø§Ø¯Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ú†Ù†Ø¯ selector Ù…Ø®ØªÙ„Ù
        print("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§...")
        
        search_selectors = [
            "._cu5ae4",  # Selector ÛŒØ§ÙØª Ø´Ø¯Ù‡ Ø¯Ø± ØªØ³Øª
            "input[placeholder*='ÕˆÖ€Õ¸Õ¶Õ¥Õ¬']",  # Ø§Ø±Ù…Ù†ÛŒ
            "input[type='text']",  # Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØªØ±
            "input[type='search']",
            "input[placeholder*='ĞŸĞ¾Ğ¸ÑĞº']",
            "input[placeholder*='Search']",
            "input[class*='search']",
            "input[name='searchQueryInput']",
            ".search-form__input",
            "[data-testid='search-input']"
        ]
        
        search_box = None
        for selector in search_selectors:
            try:
                search_box = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                print(f"âœ… Ú©Ø§Ø¯Ø± Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ selector '{selector}' ÛŒØ§ÙØª Ø´Ø¯")
                break
            except TimeoutException:
                continue
        
        if not search_box:
            print("âŒ Ú©Ø§Ø¯Ø± Ø¬Ø³ØªØ¬Ùˆ ÛŒØ§ÙØª Ù†Ø´Ø¯. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù… input Ù‡Ø§...")
            inputs = driver.find_elements(By.TAG_NAME, "input")
            print(f"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ {len(inputs)} input ÛŒØ§ÙØª Ø´Ø¯")
            for i, inp in enumerate(inputs[:5]):  # ÙÙ‚Ø· 5 Ø§ÙˆÙ„ÛŒ Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                print(f"   Input {i+1}: type={inp.get_attribute('type')}, class={inp.get_attribute('class')}")
            raise Exception("Ú©Ø§Ø¯Ø± Ø¬Ø³ØªØ¬Ùˆ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ - Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ debug_page_loaded.png Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯")
        
        # ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ù‡ Ø¬Ø³ØªØ¬Ùˆ
        search_box.clear()
        search_box.send_keys(SEARCH_QUERY)
        time.sleep(2)
        
        # Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ø¯Ú©Ù…Ù‡ Ø¬Ø³ØªØ¬Ùˆ ÛŒØ§ ÙØ´Ø§Ø± Ø¯Ø§Ø¯Ù† Enter
        from selenium.webdriver.common.keys import Keys
        search_box.send_keys(Keys.RETURN)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†ØªØ§ÛŒØ¬
        time.sleep(5)
        
        # Ø°Ø®ÛŒØ±Ù‡ screenshot Ù†ØªØ§ÛŒØ¬
        driver.save_screenshot("debug_search_results.png")
        print("ğŸ“¸ Screenshot Ù†ØªØ§ÛŒØ¬ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: debug_search_results.png")
        
        print("âœ… Ø¬Ø³ØªØ¬Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        return True
        
    except TimeoutException as e:
        print(f"âŒ Ø®Ø·Ø§: ØµÙØ­Ù‡ Ø¯Ø± Ø²Ù…Ø§Ù† Ù…Ù‚Ø±Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯")
        print(f"   Ø¬Ø²Ø¦ÛŒØ§Øª: {str(e)}")
        print("ğŸ’¡ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø³Ø§ÛŒØª 2GIS Ø¯Ø± Ú©Ø´ÙˆØ± Ø´Ù…Ø§ ÙÛŒÙ„ØªØ± Ø§Ø³Øª ÛŒØ§ Ø§ØªØµØ§Ù„ Ø§ÛŒÙ†ØªØ±Ù†Øª Ú©Ù†Ø¯ Ø§Ø³Øª")
        print("   Ù„Ø·ÙØ§Ù‹ Ø§Ø² VPN Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² API scraper Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬Ùˆ: {e}")
        driver.save_screenshot("debug_error.png")
        print("ğŸ“¸ Screenshot Ø®Ø·Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: debug_error.png")
        return False


def scroll_results(driver: webdriver.Chrome, max_scrolls: int = 50):
    """
    Ø§Ø³Ú©Ø±ÙˆÙ„ Ù„ÛŒØ³Øª Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±
    
    Args:
        driver: WebDriver Ù…Ø±ÙˆØ±Ú¯Ø±
        max_scrolls: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ú©Ø±ÙˆÙ„
    """
    print("ğŸ“œ Ø§Ø³Ú©Ø±ÙˆÙ„ Ù„ÛŒØ³Øª Ù†ØªØ§ÛŒØ¬...")
    
    try:
        # ÛŒØ§ÙØªÙ† container Ù„ÛŒØ³Øª Ù†ØªØ§ÛŒØ¬
        results_container = driver.find_element(By.CSS_SELECTOR, "[class*='scroll'], [class*='list'], [class*='results']")
        
        last_height = driver.execute_script("return arguments[0].scrollHeight", results_container)
        scrolls = 0
        
        while scrolls < max_scrolls:
            # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ†
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", results_container)
            time.sleep(1.5)
            
            # Ø¨Ø±Ø±Ø³ÛŒ ØªØºÛŒÛŒØ± Ø§Ø±ØªÙØ§Ø¹
            new_height = driver.execute_script("return arguments[0].scrollHeight", results_container)
            
            if new_height == last_height:
                print("âœ… Ù‡Ù…Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯")
                break
                
            last_height = new_height
            scrolls += 1
            
            if scrolls % 10 == 0:
                print(f"   Ø§Ø³Ú©Ø±ÙˆÙ„ {scrolls}...")
                
    except NoSuchElementException:
        print("âš ï¸  container Ù†ØªØ§ÛŒØ¬ ÛŒØ§ÙØª Ù†Ø´Ø¯ - Ø§Ø² Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        
        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ù„ ØµÙØ­Ù‡
        for i in range(max_scrolls):
            driver.execute_script("window.scrollBy(0, 1000)")
            time.sleep(1)


def extract_cafe_list(driver: webdriver.Chrome) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒØ³Øª Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ
    
    Args:
        driver: WebDriver Ù…Ø±ÙˆØ±Ú¯Ø±
        
    Returns:
        Ù„ÛŒØ³Øª URLÙ‡Ø§ÛŒ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§
    """
    print("ğŸ“ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒØ³Øª Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§...")
    cafe_urls = []
    
    try:
        # Scroll Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù‡Ù…Ù‡ Ù†ØªØ§ÛŒØ¬
        scroll_results(driver)
        time.sleep(2)
        
        # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ú©Ø§ÙÙ‡
        # Ø§ÛŒÙ† selector Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø³Ø§Ø®ØªØ§Ø± HTML Ø³Ø§ÛŒØª
        cafe_elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='/firm/'], article a, [class*='searchResults'] a")
        
        for element in cafe_elements:
            href = element.get_attribute("href")
            if href and "/firm/" in href and href not in cafe_urls:
                cafe_urls.append(href)
                
                if len(cafe_urls) >= MAX_CAFES:
                    print(f"âš ï¸  Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ ({MAX_CAFES}) - Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
                    break
        
        print(f"âœ… {len(cafe_urls)} Ú©Ø§ÙÙ‡ ÛŒØ§ÙØª Ø´Ø¯")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒØ³Øª: {e}")
    
    return cafe_urls


def extract_cafe_details(driver: webdriver.Chrome, url: str) -> Dict[str, Any]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª ÛŒÚ© Ú©Ø§ÙÙ‡ Ø§Ø² ØµÙØ­Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª
    
    Args:
        driver: WebDriver Ù…Ø±ÙˆØ±Ú¯Ø±
        url: Ø¢Ø¯Ø±Ø³ ØµÙØ­Ù‡ Ú©Ø§ÙÙ‡
        
    Returns:
        Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÙ‡
    """
    cafe_info = {
        "url": url,
        "name": "",
        "address": "",
        "phone": "",
        "website": "",
        "latitude": "",
        "longitude": "",
        "working_hours": "",
        "rating": "",
        "reviews_count": ""
    }
    
    try:
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        time.sleep(2)
        
        # Ù†Ø§Ù… Ú©Ø§ÙÙ‡
        try:
            name = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1, [class*='title'], [class*='firmName']")))
            cafe_info["name"] = name.text.strip()
        except:
            pass
        
        # Ø¢Ø¯Ø±Ø³
        try:
            address = driver.find_element(By.CSS_SELECTOR, "[class*='address'], [itemprop='address']")
            cafe_info["address"] = address.text.strip()
        except:
            pass
        
        # Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ†
        try:
            phone = driver.find_element(By.CSS_SELECTOR, "a[href^='tel:'], [class*='phone']")
            cafe_info["phone"] = phone.text.strip()
        except:
            pass
        
        # ÙˆØ¨â€ŒØ³Ø§ÛŒØª
        try:
            website = driver.find_element(By.CSS_SELECTOR, "[class*='website'] a, a[class*='link'][href*='http']")
            cafe_info["website"] = website.get_attribute("href")
        except:
            pass
        
        # Ø³Ø§Ø¹Ø§Øª Ú©Ø§Ø±ÛŒ
        try:
            schedule = driver.find_element(By.CSS_SELECTOR, "[class*='schedule'], [class*='workingHours']")
            cafe_info["working_hours"] = schedule.text.strip().replace("\n", " | ")
        except:
            pass
        
        # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        try:
            rating = driver.find_element(By.CSS_SELECTOR, "[class*='rating'], [itemprop='ratingValue']")
            cafe_info["rating"] = rating.text.strip()
        except:
            pass
        
        # ØªØ¹Ø¯Ø§Ø¯ Ù†Ø¸Ø±Ø§Øª
        try:
            reviews = driver.find_element(By.CSS_SELECTOR, "[class*='reviews'], [class*='reviewsCount']")
            cafe_info["reviews_count"] = reviews.text.strip()
        except:
            pass
        
        # Ù…Ø®ØªØµØ§Øª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ø§Ø² URL
        if "geo/" in url:
            coords = url.split("geo/")[-1].split("/")[0]
            if "," in coords:
                parts = coords.split(",")
                cafe_info["latitude"] = parts[0]
                cafe_info["longitude"] = parts[1] if len(parts) > 1 else ""

        # ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ
        try:
            # 1. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø§Ø² meta tag (Ø¨Ù‡ØªØ±ÛŒÙ† Ú©ÛŒÙÛŒØª)
            try:
                meta_img = driver.find_element(By.CSS_SELECTOR, 'meta[property="og:image"]')
                img_src = meta_img.get_attribute("content")
                if img_src:
                    cafe_info["image_url"] = img_src
            except:
                pass

            # 2. Ø§Ú¯Ø± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ø¹Ú©Ø³ Ø¯Ø± ØµÙØ­Ù‡
            if not cafe_info.get("image_url"):
                images = driver.find_elements(By.CSS_SELECTOR, "div[class*='sidebar'] img, article img")
                for img in images:
                    src = img.get_attribute("src")
                    if src and "http" in src and "icon" not in src and "logo" not in src:
                        cafe_info["image_url"] = src
                        break
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ÛŒ ØªØµÙˆÛŒØ±: {e}")
        
    except Exception as e:
        print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª {url}: {e}")
    
    return cafe_info


def save_to_json(cafes: List[Dict[str, Any]], filename: str):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ±Ù…Øª JSON"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(cafes, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")


def save_to_csv(cafes: List[Dict[str, Any]], filename: str):
    """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ±Ù…Øª CSV"""
    if not cafes:
        print("âš ï¸  Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        return
    
    fieldnames = cafes[0].keys()
    
    with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(cafes)
    
    print(f"ğŸ’¾ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    print("=" * 60)
    print("ğŸ‡¦ğŸ‡² Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±ÙˆØ§Ù† Ø§Ø² 2GIS (Selenium)")
    print("=" * 60)
    
    driver = None
    
    try:
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±
        driver = setup_driver()
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§
        if not search_cafes(driver):
            return
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒØ³Øª Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§
        cafe_urls = extract_cafe_list(driver)
        
        if not cafe_urls:
            print("\nâŒ Ù‡ÛŒÚ† Ú©Ø§ÙÙ‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return
        
        print(f"\nâœ… {len(cafe_urls)} Ú©Ø§ÙÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª ÛŒØ§ÙØª Ø´Ø¯")
        print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª...")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª Ù‡Ø± Ú©Ø§ÙÙ‡
        all_cafes = []
        for i, url in enumerate(cafe_urls, 1):
            print(f"   [{i}/{len(cafe_urls)}] {url}")
            cafe_info = extract_cafe_details(driver, url)
            all_cafes.append(cafe_info)
            time.sleep(1)  # ØªØ§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        print("\nğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        save_to_json(all_cafes, OUTPUT_JSON)
        save_to_csv(all_cafes, OUTPUT_CSV)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
        print("\n" + "=" * 60)
        print("ğŸ“Š Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ:")
        print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§: {len(all_cafes)}")
        print(f"   â€¢ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ†: {sum(1 for c in all_cafes if c['phone'])}")
        print(f"   â€¢ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ ÙˆØ¨â€ŒØ³Ø§ÛŒØª: {sum(1 for c in all_cafes if c['website'])}")
        print(f"   â€¢ Ú©Ø§ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ: {sum(1 for c in all_cafes if c['rating'])}")
        print("=" * 60)
        print(f"\nâœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
        print(f"ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ: {OUTPUT_JSON}, {OUTPUT_CSV}")
        
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
        
    finally:
        if driver:
            print("\nğŸ”’ Ø¨Ø³ØªÙ† Ù…Ø±ÙˆØ±Ú¯Ø±...")
            driver.quit()


if __name__ == "__main__":
    main()
